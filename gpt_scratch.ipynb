{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50bff0e7-1d91-4689-9843-bce60fab72ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import time\n",
    "\n",
    "#TODO Cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9288e50d-4cfb-4d18-8738-15310795b5cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hyperparameters\n",
    "batch_size = 64 # how many independent sequences will we process in parallel?\n",
    "block_size = 256 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 1000\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbd8dfd5-53ef-4f50-bf5e-81e647486712",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8c0d26-c29e-4088-abf7-dc1aacf68a86",
   "metadata": {},
   "source": [
    "# Data exploration and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a89ca96-d946-427e-aff0-dc8395997a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset: 1115394\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of dataset:\", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89139349-4cc7-4be1-b3ea-5a4a111f5cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 1000 characters\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"First 1000 characters\")\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c8ba401-a0c7-4dbc-87a2-af96931e0e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bd889aa-b264-44b7-922c-c8a198f2c1f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "# Create a mapping from characters to integeres (Tokenizer)\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s]          #ENCODER: string->integer\n",
    "decode = lambda l: ''.join([itos[i] for i in l])  #DECODER: integer->string\n",
    "\n",
    "print(encode(\"hii there\"))\n",
    "print(decode(encode(\"hii there\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40a73874-6135-4df2-bb14-c0a0a065988c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0cd15637-7d8c-4501-bf5d-62c07bfb7ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539ded8d-30af-4fd1-b262-3703cc9dd22c",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3cb22aa8-e732-4e1c-ac46-d3e953e437cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate a small batch of input x and target y\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc7b7043-a519-4160-8807-7959fe776d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Get actual batch size, sequence length, and embedding dimension from input\n",
    "        B, T, C = x.shape\n",
    "        \n",
    "        # Use self.key instead of undefined key variable\n",
    "        k = self.key(x)   # (B, T, head_size)\n",
    "        q = self.query(x) # (B, T, head_size)\n",
    "        \n",
    "        # compute attention scores\n",
    "        wei = q @ k.transpose(-2, -1) # (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        \n",
    "        # Perform weighted aggregation of the values\n",
    "        v = self.value(x) # (B, T, head_size)\n",
    "        out = wei @ v     # (B, T, head_size)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e088bbc-ca1d-431a-8d51-f21dc49831fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1297613c-b939-4d75-bb2e-efef10edf93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4*n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*n_embd, n_embd),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a83a00ca-f238-4568-a023-8b7481509eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5af0c04f-ede5-4ced-9866-ba1dac3cef74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class miniGPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        \n",
    "        # Get embeddings\n",
    "        tok_embd = self.token_embedding_table(idx) # (B, T, C)\n",
    "        pos_embd = self.position_embedding_table(torch.arange(T, device=device)) # (T, C)\n",
    "        x = tok_embd + pos_embd # (B, T, C)\n",
    "        \n",
    "        # Apply self-attention\n",
    "        x = self.blocks(x)\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:] if idx.size(1) > block_size else idx\n",
    "            \n",
    "            # get predictions\n",
    "            logits, _ = self(idx_cond)\n",
    "            \n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            \n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            \n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            \n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        \n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6216c840-237b-4a82-b949-92bc8a20ecef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.788929 M parameters\n"
     ]
    }
   ],
   "source": [
    "model = miniGPT()\n",
    "m = model.to(device)\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "129d12a4-3265-4fc6-8e5a-f922a145ea80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264b10e1-9d70-4488-8595-9b9be9048d9f",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d19007c3-8a6e-4e48-9eb7-85a376048477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "step 0: train loss 4.4948, val loss 4.4894\n",
      "------------\n",
      "\n",
      "SaVxux&f&&;gXYUCZo\n",
      "\n",
      "DhDigFmHHpkwkBj!'zqSbUrDynIP.'qZU.nIOMm&LHKHP;iqkjnH nLRT!PBC!DAyCaDbu$hb&KIGEJr.Ca-!$SrSgzoQCZcn\n",
      "yw!Ku'WP.PKxFxdFfRjI.D\n",
      "iOmLBQzFmy\n",
      "Ib, Oi\n",
      "ErLiEk!cQjZrujxHmshIbMUTrTpvrnjwF:?ag.cpa\n",
      "------------\n",
      "\n",
      "step 1000: train loss 1.5439, val loss 1.7256\n",
      "------------\n",
      "\n",
      "Went when we There sahwe; ye recourself.\n",
      "\n",
      "BENVOLINGBROKE:\n",
      "I'll not eyes, the proff, since say.\n",
      "\n",
      "QUEEN ELIZABET:\n",
      "And I be caust Edward graou\n",
      "'t am behousat hone not you the?\n",
      "\n",
      "GSAMPTER:\n",
      "Lood sance now d\n",
      "------------\n",
      "\n",
      "step 2000: train loss 1.3411, val loss 1.5587\n",
      "------------\n",
      "\n",
      "What haFird never shands; where hath these\n",
      "pamising strichly sook to cam to head; 'tis join\n",
      "holy follower thus fair, a love in York of;\n",
      "And then pardon use.\n",
      "\n",
      "LUCIO:\n",
      "As I trember's is was the lord rise\n",
      "------------\n",
      "\n",
      "step 3000: train loss 1.2402, val loss 1.5035\n",
      "------------\n",
      "\n",
      "Rest vow revense the general which thrival,\n",
      "To see himbrace all: I'll see him out\n",
      "For't: I have someture to Romeo.\n",
      "\n",
      "ROMEO:\n",
      "Now, know it with rich thither: for these great, I kneel, as blood,\n",
      "And show \n",
      "------------\n",
      "\n",
      "step 4000: train loss 1.1694, val loss 1.4841\n",
      "------------\n",
      "\n",
      "Now, but, gilty Marcius.\n",
      "\n",
      "KING HENRY VI:\n",
      "Alack, Tybalt up, friend, day the sengered;\n",
      "For whoe its to fair anchestments thy crown,\n",
      "Mispoted like post: pronounce it be England\n",
      "I'll say\n",
      "The courrention o\n",
      "------------\n",
      "\n",
      "step 4999: train loss 1.1066, val loss 1.4779\n",
      "------------\n",
      "\n",
      "No aetrath thank you, the merits even alle,\n",
      "Do witch my brother wounds troop, these very\n",
      "sight against the queening Butcherief, and tell\n",
      "Thy common than his time up with to thee.\n",
      "\n",
      "MENENIUS:\n",
      "How now! m\n",
      "------------\n",
      "Execution time: 1133.547484 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"\\nstep {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        print(\"------------\")\n",
    "        generated = model.generate(context, max_new_tokens=200)\n",
    "        print(decode(generated[0].tolist()))\n",
    "        print(\"------------\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Execution time: {elapsed_time:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdea3858-da79-4518-a41f-23f4933f0dfd",
   "metadata": {},
   "source": [
    "# Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8aad7eff-dc2c-4181-bb96-a8baa80d4ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Diddling with surmised, what born butcher pays,\n",
      "On he after, embracement. I fear to kill my tongue,\n",
      "Farewell will not be so means of money.\n",
      "My lord, have fear'd what I dreathed that broken in his\n",
      "Deconcile and rease before the common's fault: their\n",
      "I am never scarled, would I be the merry too;\n",
      "And yea, to have learn'd your so melt folt.\n",
      "\n",
      "DUKE OF AUMERLE:\n",
      "Thine own gates can,\n",
      "Are no little satisfied her;\n",
      "Ard-every mind of rebells, and he's eneminate\n",
      "With Lady Bona spark: therefore wixt the best\n",
      "I restore the beholder.\n",
      "Madam, Prisa your Trium.\n",
      "\n",
      "LADY ANNE:\n",
      "Still, fom you thit make not: I am indeed,\n",
      "that your loss master, for I keep my shorout daughter.\n",
      "\n",
      "CAPULET:\n",
      "What news' Why, are, upon action!\n",
      "Even never her voice, besides, and to cryining\n",
      "By tries!\n",
      "\n",
      "Post:\n",
      "What's marrier, your lord.\n",
      "\n",
      "WARWICK:\n",
      "What may shall does verit\n",
      "That go your honour, could hold him so mucal spers,\n",
      "Nay, here's a money; therefore in puty title;\n",
      "But for thy wolve I prove to shrift herward,\n",
      "To-morrows; he'll follow your Romeo, whence,\n",
      "Four brook and unnatural, or Power you wow,\n",
      "Our dremble Hastings and take it died.\n",
      "He was you alld, in\n",
      "himsequate and soldier, little be this brother, as a\n",
      "gat from he offence the gates, tiger would have\n",
      "the that loss than the ship and grief a\n",
      "pieces  of, and falsehood, intelling fortune,\n",
      "all mockery his pardon's heart in your highness armsely.\n",
      "\n",
      "KING RICHARD III:\n",
      "Stand be well; in far to be temporated,\n",
      "To catch the achole, or your first's good\n",
      "auth ruthed, and you are for the tyrant,\n",
      "Me may tribune make your general body,\n",
      "Beated in all bid with her, shall brave.\n",
      "\n",
      "SUMNIA:\n",
      "Menethought and power in her heads!\n",
      "Your base thou well accounter-trembling sobed.\n",
      "\n",
      "RATCLIFF:\n",
      "Amen, convention, temportuous hand;\n",
      "If--\n",
      "'Tis court-cup in body; I tell your own, and what\n",
      "If heart is a man darling contract to tell him\n",
      "As walking not-death from the freedom royal sedpite.\n",
      "That's that whose pencensio, that cruel he.\n",
      "I would you Clarence proper'd for this discover,\n",
      "And often all where I must.\n"
     ]
    }
   ],
   "source": [
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "\n",
    "# Generate text\n",
    "generated = model.generate(context, max_new_tokens=2000)\n",
    "print(decode(generated[0].tolist()))\n",
    "\n",
    "# save\n",
    "#open('more.txt', 'w').write(decode(m.generate(context, max_new_tokens=10000)[0].tolist())) # -w to file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc43cf7e-9812-44a7-be9a-387109447560",
   "metadata": {},
   "source": [
    "# Demonstration of token input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c426fb09-c6cd-43f5-94c8-79abefb66660",
   "metadata": {},
   "outputs": [],
   "source": [
    "B,T,C = 4,8,2 #batch,time,channel\n",
    "x = torch.randn(B,T,C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b371cb-fd14-443b-8960-a73eb1627408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 1, slow\n",
    "xbow = torch.zeros((B,T,C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, :t+1]\n",
    "        xbow[b,t] = torch.mean(xprev, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37e518f-7536-49cb-9fbc-158e2a9408ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tril(torch.ones(3,3))\n",
    "a = a / torch.sum(a, 1, keepdim=True)\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b # matrix multiplication\n",
    "print('a=')\n",
    "print(a)\n",
    "print('---')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('---')\n",
    "print('c=')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4b08cc-8905-44d2-a23e-5bc80e36fbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 2\n",
    "torch.manual_seed(1337)\n",
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "xbow2 = wei @ x # (Bash, T, T) @ (B, T, C) ---> (B, T, C)\n",
    "torch.allclose(xbow[0], xbow2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7318b09d-918c-474b-9f9d-11c012b3f943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 3: Use softmax\n",
    "torch.manual_seed(1337)\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "wei = torch.zeros((T, T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=1)\n",
    "xbow3 = wei @ x\n",
    "torch.allclose(xbow[0], xbow3[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3cd9e997-fe44-45e3-a680-3c6dbe8d5aac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.5713e-01,  8.8009e-01,  1.6152e-01, -7.8239e-01, -1.4289e-01,\n",
       "           7.4676e-01,  1.0068e-01, -5.2395e-01, -8.8726e-01,  1.9068e-01,\n",
       "           1.7616e-01, -5.9426e-01, -4.8124e-01, -4.8598e-01,  2.8623e-01,\n",
       "           5.7099e-01],\n",
       "         [ 8.3212e-01, -8.1437e-01, -3.2425e-01,  5.1913e-01, -1.2520e-01,\n",
       "          -4.8982e-01, -5.2867e-01, -3.1393e-02,  1.0723e-01,  8.2692e-01,\n",
       "           8.1322e-01, -2.7132e-02,  4.7754e-01,  4.9801e-01, -1.3769e-01,\n",
       "           1.4025e+00],\n",
       "         [ 6.0346e-01, -2.4995e-01, -6.1588e-01,  4.0678e-01,  3.3283e-01,\n",
       "          -3.9097e-01,  1.3119e-01,  2.1718e-01, -1.2991e-01, -8.8281e-01,\n",
       "           1.7242e-01,  4.6522e-01, -4.2710e-01, -7.6754e-02, -2.8524e-01,\n",
       "           1.3875e+00],\n",
       "         [ 6.6568e-01, -7.0960e-01, -6.0986e-01,  4.3484e-01,  8.9754e-01,\n",
       "          -9.2983e-01,  6.8324e-02,  1.8632e-01,  5.4002e-01,  2.4271e-01,\n",
       "          -6.9225e-01,  4.9774e-01,  4.8503e-01,  6.6076e-01,  8.7671e-01,\n",
       "           7.4567e-02],\n",
       "         [ 1.5357e-01,  1.0439e+00,  8.4574e-01,  2.3882e-01,  3.0046e-01,\n",
       "           1.0516e+00,  7.6373e-01,  4.5166e-01, -7.4263e-01, -1.4395e+00,\n",
       "          -4.9412e-01, -3.7087e-01, -1.1819e+00,  1.0001e-01, -1.8065e-01,\n",
       "           5.1291e-01],\n",
       "         [-8.9198e-01,  5.7820e-02, -3.3504e-01,  8.4768e-01,  3.8764e-01,\n",
       "           1.6644e-01, -4.5871e-01, -5.9737e-01,  4.9612e-01,  6.5476e-01,\n",
       "           5.4789e-02,  9.4680e-01,  4.5108e-01,  1.1999e-01,  1.0573e+00,\n",
       "          -2.2570e-01],\n",
       "         [-4.8492e-01,  1.6553e-01, -2.2215e-01, -1.3454e-01, -8.6441e-02,\n",
       "          -6.6281e-01, -9.3596e-02,  1.0496e-01, -2.6121e-01,  1.8538e-01,\n",
       "           3.1711e-01, -1.3927e-01,  5.4862e-01, -4.0864e-01, -3.8507e-01,\n",
       "           7.1057e-01],\n",
       "         [ 2.0424e-01,  3.7717e-01, -1.1255e+00,  3.9950e-01,  1.4892e-01,\n",
       "           3.5902e-01, -1.7912e-01,  1.3732e+00,  1.5880e-01, -2.3202e-01,\n",
       "           1.6507e-01,  7.6043e-01,  3.5211e-01, -1.0864e+00, -7.9393e-01,\n",
       "          -3.0253e-01]],\n",
       "\n",
       "        [[-1.3254e+00,  1.1236e+00,  2.2927e-01, -2.9970e-01, -7.6267e-03,\n",
       "           7.9364e-01,  8.9581e-01,  3.9650e-01, -6.6613e-01, -2.1844e-01,\n",
       "          -1.3539e+00,  4.1245e-01,  9.6011e-01, -1.0805e+00, -3.9751e-01,\n",
       "          -4.4439e-01],\n",
       "         [-1.9221e-01, -4.6449e-01,  5.9880e-02,  2.8408e-01, -1.0312e-01,\n",
       "          -1.7967e-03,  1.8920e-01, -3.7337e-01, -9.8137e-02,  2.3116e-02,\n",
       "           8.5743e-01,  5.6841e-01, -2.1939e-01, -2.9158e-01, -2.0158e-01,\n",
       "          -4.6876e-01],\n",
       "         [-1.1012e+00,  9.8266e-02,  5.8596e-01, -5.6413e-03,  3.7330e-01,\n",
       "          -6.1363e-02,  2.8833e-02,  2.6230e-01,  6.4099e-01,  7.1002e-02,\n",
       "           3.6877e-01,  5.0011e-01,  7.3872e-01,  1.1909e-01,  5.4246e-01,\n",
       "           6.8950e-02],\n",
       "         [ 4.9074e-01, -2.9978e-01,  1.0949e+00,  1.0131e+00,  3.5883e-01,\n",
       "           9.5771e-01, -1.8349e-01,  1.4002e-01,  1.4243e-01,  8.0787e-01,\n",
       "          -2.4476e-01,  1.3392e-01,  2.6700e-01,  3.2605e-01,  2.0296e-01,\n",
       "           1.4967e-01],\n",
       "         [ 4.5700e-02,  1.0993e+00,  4.6545e-01, -1.5803e-01, -7.2921e-01,\n",
       "           5.8145e-01,  4.0171e-01,  1.3040e+00, -2.2263e-02,  3.9847e-01,\n",
       "           6.3218e-01, -1.4205e-01,  5.0596e-01, -2.9585e-01, -3.5306e-02,\n",
       "          -7.2087e-01],\n",
       "         [ 3.6249e-01,  3.1444e-01,  3.7844e-01, -3.3100e-01, -1.1213e+00,\n",
       "          -6.8686e-01, -6.5431e-01, -2.1805e-01, -2.6552e-01,  6.7712e-01,\n",
       "           3.9176e-01, -1.3338e+00,  3.7350e-01, -1.1095e+00,  3.7270e-01,\n",
       "          -9.3442e-01],\n",
       "         [-2.0881e-01, -7.6620e-02, -1.5674e-01,  1.4457e-01,  8.7035e-01,\n",
       "           2.1136e-01, -4.8995e-01,  2.4986e-01,  5.1811e-01,  6.6507e-01,\n",
       "           3.2814e-01,  4.6015e-01,  9.2321e-01, -4.5579e-01, -4.8577e-01,\n",
       "          -2.7199e-01],\n",
       "         [-1.8408e-01,  1.7347e-01,  1.4034e-02, -4.8221e-01, -5.2118e-01,\n",
       "          -2.6668e-01, -1.0874e-01,  2.0809e-01,  3.0165e-01,  5.3594e-02,\n",
       "          -3.7746e-01, -7.4163e-01,  8.8692e-04, -1.2250e+00,  3.0022e-01,\n",
       "          -5.0357e-01]],\n",
       "\n",
       "        [[ 6.8925e-02,  1.2248e+00, -4.1194e-01, -1.7046e-01, -6.9224e-01,\n",
       "          -2.9201e-01,  1.2704e+00, -6.8596e-01,  4.3798e-01, -2.6366e-01,\n",
       "           1.1528e-01,  1.1676e+00, -7.2138e-01, -1.2308e+00,  8.3821e-01,\n",
       "          -5.5987e-01],\n",
       "         [-9.5939e-01,  9.2166e-02,  7.7470e-02, -9.8325e-02, -5.0263e-01,\n",
       "          -7.0076e-01, -7.3248e-01,  1.8081e-02,  4.7626e-01, -1.1356e-01,\n",
       "           2.6368e-01, -3.6124e-01, -2.1905e-02, -3.4626e-01, -1.0357e-01,\n",
       "           6.5548e-01],\n",
       "         [-5.7584e-01, -3.0022e-01, -6.9503e-02, -9.9645e-02, -2.8187e-01,\n",
       "          -6.7841e-01, -1.4310e-01, -3.7591e-01,  5.7496e-01,  4.6750e-04,\n",
       "           9.1726e-01,  1.6101e-01, -4.4098e-01,  5.3700e-03,  7.9788e-01,\n",
       "           5.6693e-01],\n",
       "         [ 3.4514e-01,  3.0841e-01,  1.0998e-01, -2.6316e-01,  1.0666e+00,\n",
       "          -5.6067e-02, -6.9560e-01,  3.0091e-01, -2.7255e-01,  8.2122e-01,\n",
       "          -8.6185e-01,  6.1082e-02, -1.2083e-01,  4.1112e-01, -1.0277e-01,\n",
       "          -2.9790e-01],\n",
       "         [-1.8289e+00, -8.6379e-01, -7.9821e-01,  2.4173e-01, -5.0344e-01,\n",
       "          -1.0447e+00,  8.7287e-01,  5.0584e-01,  5.6657e-02, -3.1938e-01,\n",
       "           1.0980e+00,  1.1729e+00, -5.4148e-01, -1.0805e+00,  7.3217e-02,\n",
       "          -2.8329e-01],\n",
       "         [-3.5718e-01, -3.2740e-01, -6.9867e-01,  7.8014e-01,  4.2778e-01,\n",
       "           3.3665e-01,  5.5142e-02,  5.9465e-01,  6.4841e-01, -8.7773e-02,\n",
       "          -4.3907e-02,  6.5681e-01,  1.2646e-01,  2.5969e-01,  6.7423e-01,\n",
       "          -7.6637e-01],\n",
       "         [ 7.6206e-01,  4.9035e-01,  8.2749e-01,  3.7294e-01, -7.1975e-01,\n",
       "          -3.3127e-01, -8.6443e-01, -1.6570e-03, -5.9054e-01,  6.3868e-01,\n",
       "           2.2889e-01, -5.5488e-02,  2.9504e-01,  5.3679e-01, -7.7014e-01,\n",
       "           4.9259e-01],\n",
       "         [ 4.3940e-01,  2.4456e-01, -6.1958e-01,  5.1417e-01,  8.1137e-01,\n",
       "           2.7439e-01,  1.6661e-01,  5.0555e-02,  9.1574e-02,  8.9894e-01,\n",
       "          -1.0681e-01,  3.1970e-01, -7.3390e-02,  3.0807e-01,  7.9702e-01,\n",
       "           7.5018e-01]],\n",
       "\n",
       "        [[ 9.7183e-02,  5.7301e-02, -1.0468e-01, -4.6654e-02, -1.4006e-01,\n",
       "          -8.4126e-01, -1.3625e-01, -6.7465e-01, -2.1541e-01,  1.0993e+00,\n",
       "           2.3427e-01,  3.2605e-02, -1.8521e-01,  1.4780e-01, -6.1045e-01,\n",
       "           1.5391e+00],\n",
       "         [ 3.6123e-01, -6.7973e-01, -7.7090e-01,  6.4828e-01, -2.4451e-01,\n",
       "          -5.7902e-01, -1.5354e+00, -7.2195e-01, -1.8834e-01,  1.0884e-02,\n",
       "           2.3991e-01, -5.4473e-02, -1.4373e-01,  4.9291e-02, -8.8639e-01,\n",
       "           7.2397e-01],\n",
       "         [-1.0977e-01,  8.0600e-01,  8.1140e-01, -3.4001e-01, -4.5837e-01,\n",
       "           5.4328e-03,  1.3075e+00, -7.7781e-01, -6.2820e-01,  7.4216e-02,\n",
       "          -2.1868e-01,  1.8126e-01, -2.0854e-01,  6.7201e-01,  6.9363e-02,\n",
       "           9.8662e-01],\n",
       "         [ 3.0428e-01,  1.1563e+00,  1.3803e-01, -2.0818e+00, -1.0470e-01,\n",
       "           5.2292e-01,  1.2301e+00,  5.3652e-01, -9.0009e-01, -1.0794e+00,\n",
       "          -2.4331e-01,  9.7982e-04,  2.4827e-01,  4.4169e-02, -6.7854e-01,\n",
       "          -3.3345e-01],\n",
       "         [-5.3004e-01, -9.2135e-01,  3.7915e-01, -2.0732e-02,  3.7330e-01,\n",
       "          -1.6131e-01, -7.0930e-01,  4.2039e-02,  1.6151e-01,  1.6618e-01,\n",
       "           5.6694e-01,  5.5056e-01, -7.1126e-02, -5.5536e-01, -1.2077e-01,\n",
       "          -4.5284e-01],\n",
       "         [-6.9652e-01,  4.4457e-01,  8.0947e-01, -6.0359e-01,  4.7886e-02,\n",
       "          -4.6401e-01, -2.0967e-01,  5.5984e-01,  5.7196e-01,  3.6429e-01,\n",
       "           5.9383e-02, -1.3565e+00,  6.8667e-01,  5.4511e-01, -6.7370e-01,\n",
       "           6.3525e-01],\n",
       "         [ 3.5459e-01,  1.1575e-01, -4.2291e-01, -4.7040e-01, -2.2670e-01,\n",
       "           1.5671e-01, -2.1000e-01, -1.0505e+00, -1.0665e+00, -8.3185e-01,\n",
       "           1.9891e-01,  9.0778e-01,  3.5189e-01,  5.6643e-02, -6.4876e-01,\n",
       "           5.5124e-02],\n",
       "         [-1.7223e+00,  5.1077e-01,  2.9681e-01,  2.3290e-01,  2.4183e-01,\n",
       "           3.3723e-01, -2.5232e-01,  6.4762e-01, -1.4068e+00, -6.4379e-01,\n",
       "           7.4489e-02, -5.8730e-01,  1.2959e-01, -2.1585e-01, -7.5063e-01,\n",
       "           3.2311e-01]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Version 4: self-attention (!!)\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4, 8, 32\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "# Head of self-attention\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x)   # (B, T, 16)\n",
    "q = query(x) # (B, T, 16)\n",
    "wei = q @ k.transpose(-2, -1)   # (B, T, 16) @ (B, 16, T) --> (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "#wei = torch.zeros((T, T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=1)\n",
    "\n",
    "v = value(x)\n",
    "out = wei @ v\n",
    "#out = wei @ x\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "ml-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
